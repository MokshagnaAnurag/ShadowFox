# -*- coding: utf-8 -*-
"""Boston House Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HBO6j0IznDoKvrOFq3rZGaOuwpU3LJnd

Boston House Price Prediction**
Step 1: Load and Preprocess the Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from google.colab import files
uploaded = files.upload()
import io
data = pd.read_csv(io.BytesIO(uploaded['HousingData.csv']))
print(data.head())
print(data.isnull().sum())
data.fillna(data.mean(), inplace=True)
print(data.describe())
plt.figure(figsize=(12,8))
sns.boxplot(data=data)
plt.xticks(rotation=90)
plt.show()

# Print the column names to inspect them
print(data.columns)

"""# **Step 2: Feature Selection and Target Variable**"""

# Use the correct column name for the target variable, which is 'MEDV'
X = data.drop(columns=['MEDV'])  # Features
y = data['MEDV']  # Target

# Splitting the dataset into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Proceed with scaling and model training
from sklearn.preprocessing import StandardScaler

# Scaling the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Now you can proceed with your model training and evaluation

"""# 3. Model **Selection**

**Linear Regression:**
"""

# Importing Linear Regression and metrics
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the Linear Regression model
lr_model = LinearRegression()

# Train the model
lr_model.fit(X_train_scaled, y_train)

# Predict on the test data
y_pred_lr = lr_model.predict(X_test_scaled)

# Evaluate the model
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print(f"Linear Regression Mean Squared Error: {mse_lr}")
print(f"Linear Regression R-squared Score: {r2_lr}")

"""**Decision Tree Regressor:**"""

from sklearn.tree import DecisionTreeRegressor

# Initialize Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)

# Train the model
dt_model.fit(X_train_scaled, y_train)

# Predict on the test data
y_pred_dt = dt_model.predict(X_test_scaled)

# Evaluate the model
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)

print(f"Decision Tree Mean Squared Error: {mse_dt}")
print(f"Decision Tree R-squared Score: {r2_dt}")

"""**Gradient Boosting Regressor:**"""

from sklearn.ensemble import GradientBoostingRegressor

# Initialize Gradient Boosting Regressor
gbr_model = GradientBoostingRegressor(random_state=42)

# Train the model
gbr_model.fit(X_train_scaled, y_train)

# Predict on the test data
y_pred_gbr = gbr_model.predict(X_test_scaled)

# Evaluate the model
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"Gradient Boosting Mean Squared Error: {mse_gbr}")
print(f"Gradient Boosting R-squared Score: {r2_gbr}")

"""# **Step 4:Evaluation**"""

# Evaluation of Linear Regression
print(f"Linear Regression MSE: {mse_lr}")
print(f"Linear Regression R²: {r2_lr}")

# Evaluation of Decision Tree
print(f"Decision Tree MSE: {mse_dt}")
print(f"Decision Tree R²: {r2_dt}")

# Evaluation of Gradient Boosting
print(f"Gradient Boosting MSE: {mse_gbr}")
print(f"Gradient Boosting R²: {r2_gbr}")

"""# **Step 5:Fine-Tuning**"""

# Fine-tuning the Gradient Boosting Regressor
gbr_tuned = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=4, random_state=42)
gbr_tuned.fit(X_train_scaled, y_train)

# Predict on the test data
y_pred_gbr_tuned = gbr_tuned.predict(X_test_scaled)

# Evaluate the fine-tuned model
mse_gbr_tuned = mean_squared_error(y_test, y_pred_gbr_tuned)
r2_gbr_tuned = r2_score(y_test, y_pred_gbr_tuned)

print(f"Tuned Gradient Boosting MSE: {mse_gbr_tuned}")
print(f"Tuned Gradient Boosting R²: {r2_gbr_tuned}")

"""# **Step 6:Hyperparameter Tuning**

"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
}

# Initialize Gradient Boosting Regressor
gbr = GradientBoostingRegressor(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)

# Fit Grid Search
grid_search.fit(X_train_scaled, y_train)

# Best parameters and score
print("Best parameters found: ", grid_search.best_params_)
print("Best score found: ", -grid_search.best_score_)

# Check for missing values in the original dataset
print(data.isnull().sum())

"""**Randomized Search**"""

from sklearn.impute import SimpleImputer

# Initialize the imputer
imputer = SimpleImputer(strategy='mean')

# Fit and transform the training data
X_imputed = imputer.fit_transform(X)

# Split the imputed data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)

# Now scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# **Step 7:Cross-Validation**"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Define the parameter distributions
param_dist = {
    'n_estimators': randint(100, 300),
    'learning_rate': uniform(0.01, 0.2),
    'max_depth': randint(3, 6),
    'min_samples_split': randint(2, 11),
}

# Initialize Gradient Boosting Regressor
gbr = GradientBoostingRegressor(random_state=42)

# Initialize Randomized Search
random_search = RandomizedSearchCV(estimator=gbr, param_distributions=param_dist, n_iter=50, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1, random_state=42)

# Fit Randomized Search
random_search.fit(X_train_scaled, y_train)

# Best parameters and score
print("Best parameters found: ", random_search.best_params_)
print("Best score found: ", -random_search.best_score_)

"""# **Step 8:Feature Engineering**"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingRegressor

# Initialize Gradient Boosting Regressor
gbr = GradientBoostingRegressor(random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(gbr, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')

print("Cross-Validation MSE Scores: ", -cv_scores)
print("Mean CV MSE: ", -cv_scores.mean())
print("Standard Deviation of CV MSE: ", cv_scores.std())

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_train_scaled)

# Use these features for training
X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_poly, y_train, test_size=0.2, random_state=42)

from sklearn.feature_selection import SelectFromModel

# Fit a model to get feature importances
gbr.fit(X_train_scaled, y_train)
model = SelectFromModel(gbr, threshold="mean", prefit=True)

# Transform the features
X_train_selected = model.transform(X_train_scaled)
X_test_selected = model.transform(X_test_scaled)

# Example interaction feature creation
data['RM_DIS'] = data['RM'] * data['DIS']

print("X_train_scaled shape:", X_train_scaled.shape)
print("y_train shape:", y_train.shape)
